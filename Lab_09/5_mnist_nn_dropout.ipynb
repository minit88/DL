{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8dEo8U/qEuGkmnz8qOWiK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minit88/ML-DL/blob/main/Lab_09/5_mnist_nn_dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eT72kNoXTZeq"
      },
      "outputs": [],
      "source": [
        "# over fitting 된 모델들은 에러를 증가 시킬 수 있다.\n",
        "# Drop out 은 학습을 진행하면서 각 노드 뉴런이라고 불리는 이들을 설정된 무작위 비율에 따라서 껏다 켰다를 반복\n",
        "# 각 설정된 노드를 따라 아웃풋으로 나오게 되고 이 다음 backpropagation 실행 후 Drop out 반복"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random"
      ],
      "metadata": {
        "id": "_qz34nmHkwdG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "q795QRrxlBaf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "drop_prob = 0.3"
      ],
      "metadata": {
        "id": "H4PcQYFulEVR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "metadata": {
        "id": "ezxFpX2ilMO3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = torch.utils.data.DataLoader(dataset = mnist_train,batch_size=batch_size,shuffle=True,drop_last=True)"
      ],
      "metadata": {
        "id": "qK3mxgEdlZGd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
        "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "dropout = torch.nn.Dropout(p=drop_prob)"
      ],
      "metadata": {
        "id": "GZGet2_0l_4V"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xavier initialization\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)\n",
        "torch.nn.init.xavier_uniform_(linear4.weight)\n",
        "torch.nn.init.xavier_uniform_(linear5.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5-tKS89mRY6",
        "outputId": "d2dd8565-1061-46d4-cc06-2cc48b2de8bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0565,  0.0423, -0.0155,  ...,  0.1012,  0.0459, -0.0191],\n",
              "        [ 0.0772,  0.0452, -0.0638,  ...,  0.0476, -0.0638,  0.0528],\n",
              "        [ 0.0311, -0.1023, -0.0701,  ...,  0.0412, -0.1004,  0.0738],\n",
              "        ...,\n",
              "        [ 0.0334,  0.0187, -0.1021,  ...,  0.0280, -0.0583, -0.1018],\n",
              "        [-0.0506, -0.0939, -0.0467,  ..., -0.0554, -0.0325,  0.0640],\n",
              "        [-0.0183, -0.0123,  0.1025,  ..., -0.0214,  0.0220, -0.0741]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model = torch.nn.Sequential(linear1, relu, dropout,\n",
        "                            linear2, relu, dropout,\n",
        "                            linear3, relu, dropout,\n",
        "                            linear4, relu, dropout,\n",
        "                            linear5).to(device)"
      ],
      "metadata": {
        "id": "kKNi8Df6mTrR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Qp12VojnmbLm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9bSEfn2vezQ",
        "outputId": "7a4b4625-53c1-4251-d7bf-229472993680"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 cost = 0.310621142\n",
            "Epoch: 0002 cost = 0.146159276\n",
            "Epoch: 0003 cost = 0.111172289\n",
            "Epoch: 0004 cost = 0.094859935\n",
            "Epoch: 0005 cost = 0.082409069\n",
            "Epoch: 0006 cost = 0.072757378\n",
            "Epoch: 0007 cost = 0.071835004\n",
            "Epoch: 0008 cost = 0.059738025\n",
            "Epoch: 0009 cost = 0.059085678\n",
            "Epoch: 0010 cost = 0.055539340\n",
            "Epoch: 0011 cost = 0.051415320\n",
            "Epoch: 0012 cost = 0.047818515\n",
            "Epoch: 0013 cost = 0.045675199\n",
            "Epoch: 0014 cost = 0.046652373\n",
            "Epoch: 0015 cost = 0.046102792\n",
            "Learning finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hrQQuvvzpKf",
        "outputId": "01c5bf8c-c311-47d2-b2df-238a342d798a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.972599983215332\n",
            "Label:  8\n",
            "Prediction:  8\n"
          ]
        }
      ]
    }
  ]
}